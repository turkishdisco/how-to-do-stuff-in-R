---
title: "How to do stuff in R"
date: last-modified
date-format: "[Last updated:] DD/MM/YYYY HH:mm"
format:
  html:
    standalone: true
    embed-resources: true
    toc: true
    toc-depth: 3
    toc-expand: 1
    number-sections: true
    toc-location: left
    df-print: kable
    code-fold: false
    theme: default
    fontsize: 1em
    link-external-newwindow: true
execute:
  warning: false
  echo: true
  eval: false
---

A compendium of useful code snippets and recipes for wrangling and analysis in R.

Notes:

* This guide assumes familiarity with the following: basic R functions (e.g. `getwd()`, `c()`, `head()`, etc); the tidyverse suite of packages (especially `dplyr`, `tidyr`, and `readr`); and the concept of piping (`%>%`). 

* In many cases the examples here are reproducible, but sometimes code snippets are just non-reproducible examples from previous work.

* This guide is far from perfect or complete, andcontinues to be maintained and updated over time. It is stored in a personal reprository on GitHub [https://github.com/turkishdisco/how-to-do-stuff-in-R](https://github.com/turkishdisco/how-to-do-stuff-in-R) where the underlying Quarto (.qmd) file can be viewed. 

```{r}
#| label: packages
#| eval: true
#| echo: false
#| message: false
#| output: false
library(pacman)
p_load(
  tidyverse,
  janitor,
  nycflights13,
  gt,
  gtsummary,
  cardx,
  titanic,
  psych,
  rstatix,
  ggcorrplot,
  broom,
  pwr,
  pwrss)
```

# Packages {#sec-packages}

## Installing packages
If you have no intention of sharing your code, you can simply use the following to install and then load the packages you want:
```{r}
install.packages("tidyverse")
library(tidyverse)
```

Note that sometimes there may be functions from different packages that share the same name, or you may want to be explicit about where you want a function to load from. In that case, you can write the name of the package followed by two colons and then the function name, e.g:
```{r}
dplyr::select(mpg, manufacturer, model, displ)
```

If you DO intend to share your code, you may want to consider using `pacman`. This will load a package if you already have it installed, but it will also install it if not. E.g. someone else executing your code might not have all the necessary packages installed, so instead of checking and installing these manually `pacman` can handle this.
```{r}
# load the pacman package first - note, this WILL need to be installed if it isn't already
install.packages("pacman")
library(pacman)

# the main function is p_load
pacman::p_load(tidyverse)
```

## Updating packages
After updating your version of R (not RStudio), you will have a brand new directory devoid of packages you installed with the previous R version. Below is a way to find a list of all these packages and reinstall them with the new version of R (as described [here](https://stackoverflow.com/questions/61808153/installr-does-not-copy-over-packages-to-new-version-of-r)).
```{r}
# before installing, find where the current library is stored:
path = .libPaths()[1]
# this will give you the current library. You may need to go back and edit, e.g.
# change win-library/4.4 to 4.3. 

old_packages <- installed.packages(lib.loc = path)

# create a list of currently installed packages
new_packages <- installed.packages()

# create a list of the missing packages that need to be installed
missing_df <- as.data.frame(old_packages[
  !old_packages[, "Package"] %in% new_packages[, "Package"], 
])

# install missing packages
install.packages(missing_df$Package, Ncpus = 3)
```

# Read data
## Read from local storage
```{r}
# optionally state how you want NA values to be handled
df = readr::read_csv("myFile.csv", na = c(""))

df = readr::read_delim("myFile.txt", delim = "|", na = c(""))

df = readxl::read_excel("myFile.xlsx", sheet = 1, na = c(""))
# note: if this throws an error about "cannot open zip file", it's because you need to close the file
# on your desktop first.
```

Combine with `janitor::clean_names()`
```{r}
df = read_csv("myFile.csv", na = c("")) %>%
  clean_names(case = c("snake", "lower_camel", "upper_camel", ...))
```


Alternative file types
```{r}
# SPSS (.sav)
df = haven::read_sav("myFile.sav")

# JSON
df = rjson::fromJSON(file = "myFile.json")
df = as.data.frame(df)

# Password-protected files
df = excel.link::xl.read.file("myFile.xlsx", password = "my_password")
```


### Read in very large files
```{r}
# fread()
df = data.table::fread(
    input = "path/myFile.xlsx",
    header = T,
    sep = "|",  
    na.strings = "",
    encoding = "UTF-8",
    colClasses = list(
      character = c("colA", "colB", "colC"),
      numeric = c("colD", "colE")))

# fst()
# (see example of write_fst below)
df = fst::read_fst('myFile.fst')
```


## Read in from a database
```{r}
# First establish a connection 
# Example 1:
con = dbConnect(odbc::odbc(),
                Driver = "ODBC Driver 17 for SQL Server",
                Server = "mtceepr.database.windows.net",
                Database = "mtc-prod-2022",
                Authentication = "ActiveDirectoryIntegrated",
                Port = 49554)

# Example 2:
con = dbConnect(odbc::odbc(), 
                Driver = "SQL Server", 
                Server = "3DCPRI-PDB16\\ACSQLS", 
                Database = "MatchedAdminSQL",
                Trusted_Connection = "yes")

# Then load in data:
# use dplyr for more efficiency
# first step:
my_tbl = tbl(con, sql("select * FROM [PDR].[Tier1].[Phonics_MasterView]") )

# second step:
my_df = my_tbl %>% 
  filter(AcademicYear %in% c("202223"), Version=="F") %>% 
  select(AcademicYear, PupilMatchingRefAnonymous) %>% 
  collect()


# Older method I used:
# Read in entire table
df = odbc::dbGetQuery(
  ILR_con,
  "SELECT * 
  FROM table_name")

# Read in selected columns with filtering (requires SQL commands)
df = odbc::dbGetQuery(
  con,
  "SELECT [var1], [var2], [var3]
  FROM [database].[table]
  WHERE var4 >= 2 AND var7 <= 10"
)
```

It's also possible to write a SQL query and pass this as a string:
```{r}
query = paste0(
  "
  SELECT 
  
  PupilMatchingRefAnonymous AS pmr,
  max(urn) as urn,
  max(PTSBIOLG) as PTSBIOLG, 
  max(PTSCHEMG) as PTSCHEMG, 
  max(PTSCOMPG) as PTSCOMPG, 
  max(APCOMBSCI91) as APCOMBSCI91,
  max(PTSPHYG) as PTSPHYG,
  max(NFTYPE) as NFTYPE
  
  FROM [PDR].[Tier0].[KS4_Pupil2_MasterView]
  WHERE
  AcademicYear = 202122
  AND [VERSION] = 'F'
  "
)

# and then collect this
df <- collect(tbl(con, sql(KS4_query)))
```


## Data validation
### Missing values
Count number or percent of missing observations for each column
```{r}
#| eval: true
airquality %>%
  summarise(across(everything(), ~sum(is.na(.) / n() )))
```


Count missing observations across all columns
```{r}
#| eval: true
airquality %>%
  mutate(sum_na = rowSums(is.na(.))) %>%
  select(Ozone, Solar.R, Wind, sum_na) %>% head()
```

`naniar` package
```{r}
# total % of missing values
naniar::pct_miss(airquality)  # gives a numeric output eg 4.79

# % of rows with any missing values
naniar::pct_miss_case(airquality) # gives a numeric output eg 27.45
```

Visualise missingness by variable
```{r}
#| eval: true
#| fig-height: 2

# visualise missingness by variable
naniar::gg_miss_var(airquality, show_pct = T)
```

Visualise missingness by case
```{r}
#| eval: true
#| fig-height: 3
naniar::vis_miss(airquality)
```

## Interrogating data
Count distinct values
```{r}
#| eval: true
# single vector
starwars %>% summarise(homeworld_distinct = n_distinct(homeworld))

# where is.character
starwars %>% summarise(across(where(is.character), n_distinct))
```

Count duplicates
```{r}
#| eval: true
# across all columns
iris %>% janitor::get_dupes()

# across specified columns
mtcars %>% janitor::get_dupes(mpg, hp)

# with base R
sum(duplicated(mtcars$disp))
```

Retain only distinct rows (delete duplicates). This is much faster than `group_by` and `slice_tail`. 
```{r}
mtcars %>% distinct(mpg, hp, .keep_all = T)

# another option is to use first()
df %>% summarise(across(everything(), ~ first(.x))) # also available: last() and nth(n = 3)
```

Sometimes you get duplicate rows when you perform joins because the rows aren't actually distinct. This can be because some cells have missing data and others don't, so R sees them as different. In this case, you can use the code below to retain only the rows that have the least missing data.
```{r}
df %>%
  arrange(rowSums(is.na(.))) %>%
  distinct(unique_ref, .keep_all = T)
```


## Other validation

`validate` package
```{r}
# example from James Tierney
extract_rules <- validate::validator(
    # Each row must be unique in representing the teaching hours allocated to one subject 
    # in one year group for one teacher at one school
    is_unique(census_year, staff_matching_reference, la_estab, subject_description_sfr, year_group), #1
    
    # Checks for availability of records on key fields
    is_complete(census_year), #2
    is_complete(census_year, staff_matching_reference), #3
    is_complete(census_year, staff_matching_reference, la_estab), #4
    is_complete(census_year, staff_matching_reference, la_estab, subject_description_sfr), #5
    is_complete(census_year, staff_matching_reference, la_estab, subject_description_sfr, year_group), #6
    is_complete(census_year, staff_matching_reference, la_estab, subject_description_sfr, year_group, hours), #7
    
    # Hours taught must be positive and no more than 40 hours per week
    hours >= 0 & hours <= 40 #8
    )

swc_extract_validation_results <- summary(validate::confront(df, extract_rules))

extract_missing_values <- df %>% 
  summarise(across(.cols = everything(), ~sum(is.na(.x))))
```


`skimr` package
```{r}
#| eval: true
skimr::skim(starwars)
```


## Write data
```{r}
write.csv(df, "fileName", row.names = F)

readr::write_csv(df, "fileName", delim = ",", na = "")

readr::write_delim(df, "fileName", delim = "|", na = "")

# fst:
write.fst(df, "fileName.fst")

# to a database:
DBI::dbWriteTable(conn = con, 
             value = df, 
             name = "table_name",
             overwrite = T)
```


# Wrangling
## NA values
### Converting NA to another value

`dplyr::replace_na()`
```{r}
# these all reproduce the original dataframe but with amended cell values
airquality %>% mutate(Ozone = replace_na(Ozone, 0))

# examples using across()
airquality %>% mutate(across(everything(), ~ replace_na(., 0)))

iris %>% mutate(across(ends_with(".Length"), ~ replace_na(., 0)))

iris %>% mutate(across(starts_with(c("Sepal", "Petal")), ~ replace_na(., 0)))

# using across
df %>% across(c("var1", "var2", "var3"), ~if_else(is.na(.x), "Missing", .x))
```

`tidyr::fill`
This is useful if you have a value that is only inserted once due to grouping, but you need to repeat this value for all the remaining rows.
```{r}
#| eval: true
#| # notice value for row 5 is missing
airquality %>% select(Ozone) %>% head()

airquality %>%
  tidyr::fill(Ozone, .direction = "down") %>% select(Ozone) %>% head()
# note how 18 has been repeated on line 5 (repeated from above)
```


### Converting values to NA
`dplyr::na_if`
```{r}
# e.g. convert "unknown" to NA
starwars %>% mutate(eye_color = na_if(eye_color, "unknown"))

starwars %>% mutate(across(where(is.character), ~ na_if(., "unknown")))
```


### Dropping NA values
Use `dplyr::drop_na` to drop all NA values
```{r}
# Drop all
airquality %>% drop_na()

# Drop NA in specific columns
airquality %>% drop_na(Ozone, Solar.R)

# use across columns
df %>% drop_na(c(starts_with("c_")))

# Note: for large datasets, consider data.table:
# dt = as.data.table(df)
# dt[complete.cases(dt)]
```

To remove rows that contain all NA (i.e. retain rows with some non-NA content):
```{r}
airquality[rowSums(is.na(airquality)) != ncol(airquality), ]
```


## Wide to long format
This is a very common data wrangling task. The recipe below converts `starwars` to long-format to easily compute summary statistics. Firstly use `pivot_longer()`:
```{r}
#| eval: true

# original wide format (each character has a single row)
starwars %>% select(-c(films)) %>% head()

# conver to long format (each character has 3 rows)
starwars_long = starwars %>% 
  select(-c(films, vehicles, starships)) %>%
  pivot_longer(cols = c(height, mass, birth_year), 
               names_to = "measure", 
               values_to = "value")
# note that pivot_longer won't combine columns of different types - if this happens, use values_transform with a specified function
head(starwars_long) 
```

Secondly, group by `measure` and compute summary statistics.
```{r}
#| eval: true
starwars_long %>%
  group_by(measure) %>%
  summarise(min = min(value, na.rm=T),
            mean = mean(value, na.rm=T),
            max = max(value, na.rm=T))
```


## Long to wide format
This example accomplishes the opposite - converting long data to wide format. The recipe below converts `us_rent_income` from long to wide format such that each row represents a single NAME with separate columns for 'income' and 'rent'.
```{r}
#| eval: true
# original long format
us_rent_income = tidyr::us_rent_income %>% select(-c(moe))
us_rent_income %>% head()
```

```{r}
#| eval: true
us_rent_income %>%
  pivot_wider(names_from = variable,
              values_from = estimate) %>%
  head()
```

Note that the `id_cols` argument can be used if your pivot results in NAs for one level of a factor. In the above, this would be `GEOID`. 

### Melt and cast with `data.table`
```{r}
# a more complicated procedure - select columns starting with Q and a number from 1 to 31 and columns ending in 'Sco', and pivot longer by putting these values into columns called QID and Score:
melted_df = data.table::melt(df, 
                             measure = patterns("^Q([1-9]|[12][0-9]|3[01])ID$", "Sco$"),
                             value.name = c("QID", "Score"))
  
cast_df = data.table::dcast(melted_df, 
                            PupilId + FormID + group ~ QID, 
                            value.var="Score", drop=T)
```


## Separate and unnest
```{r}
#| eval: true
c_list = data.frame(
  a = c("item1,item2,item3"),
  b = c("item1,item2,item3"))

c_list

# separate items into new rows
c_list %>% tidyr::separate_longer_delim(a, delim=",")

# separate items into new cols
c_list %>% tidyr::separate_wider_delim(a, delim=",", names = c("col1", "col2", "col3"))
```

Another recipe - say you want to split this `measure` column into two columns, by the underscore delimiter and then pivot wider to see the values side by side for every measure.
```{r}
#| eval: true

df = data.frame(
  measure = c("TP_thresh", "TN_thresh", "FP_thresh", "FN_thresh", "TP_cut", "TN_cut", "FP_cut", "FN_cut"),
  value = c(41, 203, 36, 6, 42, 181, 58, 5))
  
df

df %>%
  separate_wider_delim(measure, delim = "_", names = c("metric", "type")) %>%
  pivot_wider(names_from = type, values_from = value)
```


A `tidyr::unnest` recipe: given a list of items for each row (e.g. `c("The Empire Strikes Back", "Revenge of the Sith" [...]`), extract each one into a new row.
```{r}
#| eval: true

starwars %>%
  unnest_longer(films) %>%
  select(name, films) %>% head()
```


## Unite
Recipe: combine the values in two columns
```{r}
#| eval: true
df = data.frame(
  group = c("AA", "BB", "CC"),
  size = c("big", "small", "medium"),
  value = c(20, 25, 40))

df

df %>% unite(col = "united", group, size, sep = "_")
df %>% unite("united", group, size, value, sep="-")
```


## Coalesce
`coalesce` searches for the first non-missing value across columns specified. 
```{r}
df %>% mutate(flag = coalesce(TestMark1A, TestMark2A, TestMark3A))
```


## Filter and other rowwise functions
Note that filtering is faster on ungrouped data.
```{r}
mtcars %>% filter(gear >= 4 & hp > 109)
```


### Slice
Use `slice` functions to subset rows.

Recipe: take a random sample of rows in a df:
```{r}
# subset a number of rows
slice_sample(df, n = 50, replace = F)

# subset a proportion of rows
slice_sample(df, prop = 0.5, replace = F)

# note that sample_n has been superseded by slice_sample
```


Recipe: look for duplicate entries according to certain variables and use `slice_tail` to select the last observation in each grouping. This works by assigning a row number to each row according to its arrangement/grouping, then simply retaining the last row in the group.
```{r}
#| eval: true
rownames(mtcars) = NULL

mtcars %>%
  select(mpg, cyl, disp, hp) %>%
  group_by(mpg, cyl, disp, hp) %>%
  mutate(id = row_number()) %>% head()
```

Above we see that if a row is a unique combination of mpg, cyl, disp, and hp then its id is 1. Otherwise, its id is 2. `slice_tail` will assign grouped row numbers like this and retain only the last row number in a group.
```{r}
mtcars %>%
  select(mpg, cyl, disp, hp) %>%
  group_by(mpg, cyl, disp, hp) %>%
  slice_tail(n = 1) 
```

Alternatively, use `slice(1)` in the above to select the **first** row. 

### Duplicate/replicate rows
Say you have the following df where you would like to duplicate each row according to the value in the `cnt` column:
```{r}
#| eval: true
#| echo: false
df = data.frame(
  pupil = c("A01", "A02", "A03", "A04", "A05"),
  score = c(18, 14, 15, 18, 10),
  cnt = c(1, 1, 3, 1, 2)
)
df
```

`slice` can also be used to display or print a certain number of rows from a tibble (this has superseded dplyr's `top_n`). If you are using this in combination with `tabyl`, you may need to add an `order_by` argument (see below).
```{r}
mtcars %>% slice_max(mpg, n=5)
mtcars %>% slice_min(qsec, n=5)

df %>% 
  tabyl(x, y) %>%
  slice_max(n = 5, order_by = n)

``` 

### `pmax` and `pmin`
```{r}
#| eval: true
#| echo: false
# set up data
set.seed(2024)
df = data.frame(
 A = sample(1:100, 50, replace = TRUE),
 B = sample(1:100, 50, replace = TRUE),
 C = sample(1:100, 50, replace = TRUE))
```

In this recipe, we find the minimum or maximum value for each row across a specified range of columns. Here is the data:
```{r}
#| eval: true
#| echo: true
head(df)
```
So we want to find the maximum value for each row across each of the three columns.

```{r}
#| eval: true
#| echo: true

df = df %>%
 mutate(min_val = pmin(A, B, C, na.rm=T),
        max_val = pmax(A, B, C, na.rm=T))

head(df)
```
Here we see that the min value for row 1 is 27 (col C) and the max is 66 (col A). 


## Deriving new variables
### `if_any` and `if_all`
```{r}
# if_any and if_all return a logical result, e.g. assessing whether some condition is present across 
# any or all of a specific set of columns. The examples below work for both functions. 

# Recipe: return TRUE if value is "blue" in any columns containing "color"
starwars %>% mutate(blue = if_any(
  contains("color"), ~ . == "blue"))

# Recipe: flag when any columns starting with "TotalMarkA" contain "L"
df %>% mutate(contains_L = if_any(starts_with("TotalMarkA"), ~ . == "L"))

# Recipe: state whether a value in either column appears in a list of values:
hierarchy = c("A", "B", "C", "D")
df %>% mutate(in_hierarchy = if_any(c(TestMark1, TestMark2), ~ . %in% hierarchy))
```

You can also use `if_any` with `filter`:
```{r}

# filter rows where any of Sepal.Width or Petal.Width is bigger than 4. 
iris %>%
  filter(if_any(c(Sepal.Width, Petal.Width), ~ . > 4))

# Filter out rows that contain "Legacy grade"
df %>%
  filter(if_all(c(gcse_grade_eng_lit_15,
                  gcse_grade_maths_15), ~ . != "Legacy grade"))

# Filter rows where columns are numeric and cell values are above 5
df %>%
  filter(if_any(where(is.numeric), ~ . > 5))
```


### `case_when`, `case_match`
Recipe: create a new variable to indicate "large" vs "small" starwars characters according to mass and height. 
```{r}
#| eval: true
#| 
starwars %>%
  mutate(size_group = case_when(
    mass > 100 | height > 200 ~ "large",
    T ~ "small"
  )) %>%
  select(name, mass, height, size_group) %>% head()
```

Here, `T` effectively reads as "else", i.e. to specify what happens if everything evaluates to FALSE or NA. To supply an NA value instead, you can use one of the following (making sure to align with the data type in the other labels) e.g.:

- `T ~ NA_character_`
- `T ~ NA_integer_`

Note that `case_when` needs to operate on the same class of vectors, e.g. character-only, numeric-only, etc. It's not possible to combine values of e.g. `~ 50` and `~ "large"`. 

Another example using `if_any` (this says: mark an error if any of TestMark1A to 3A are "0"):
```{r}
df %>% 
  mutate(missing_pages_check = case_when(
      if_any(c(TestMark1A, TestMark2A, TestMark3A), ~. == "0") ~ "Error",
      T ~ "OK"))
```

Note that if you want to supply a formula on the right hand side of case_when, then you need to add `rowwise` in the pipe (but don't forget to ungroup afterwards):
```{r}
df %>%
  rowwise() %>%
  mutate(new_var = case_when(
    nchar(var1) == 14 ~ str_sub(var1, 5, 8),
    T ~ NA_character_
  )) %>%
  ungroup()
```

Remember to turn off `rowwise` if you want to do any aggregation/counting/grouping, by using `ungroup()`. 

### Cross-column operations
There are many examples of the `across` function in this guide. The examples below use `across()` or `purrr:pmap`.

```{r}
# Recipe: sum values above 0 across columns containing "Recall"
df %>% mutate(var1 = pmap_int(select(., contains("Recall")), 
                              ~sum(c(...) > 0, na.rm=T)))

# Recipe: count instances of value "k" across columns containing "InputMethods"
df %>% mutate(mode_k = pmap_int(select(., contains("InputMethods")), 
                                ~sum(c(...) %in% "k")))

# change certain variables to factors
df %>% mutate(across(starts_with("c_", "y_"), ~as.factor(.x))) # also as.numeric, as.character

# round all numeric columns to 0 decimal places. You must now use an anonymous function to do this:
# where before across(a:b, mean), 
# now use across(a:b, \(x) mean(x))
iris %>%  mutate(across(where(is.numeric), \(x) round(x, 0)))

# derive a flag across cols 2 to 7 - 1 if cells are non-NA and 0 if they are NA
df %>% mutate(any_grade = if_else(rowSums(!is.na(across(2:7))) > 0, 1, 0))
```



## Changing variables
### Rename columns
```{r}
iris %>% rename(sepal_length = "Sepal.Length")

# you can also rename by index
iris %>% rename("newname" = 1)
```

`rename_with` - replace all ".x" and ".y" created by joining (these become _sc and _agg)
```{r}
df %>%
  left_join(df2, by = "CurrDfENo") %>%
  rename_with(stringr::str_replace,
              pattern = ".x",
              replacement = "_sc") %>%
  rename_with(stringr::str_replace,
              pattern = ".y",
              replacement = "_agg")
```


### Relocate
```{r}
iris %>% relocate(Species, .before = Sepal.Length)
iris %>% relocate(Petal.Width, .after = last_col())

# if you don't specify .before or .after, the column will be moved to the front by default
iris %>% relocate(Species)

# alternatively, you can just list column names to derive a new order
iris %>% relocate(Species, Petal.Width, Sepal.Length)
# any remaining column names that you don't specify will remain in the data in their original
# positions relative to each other

# you can also rename while you relocate
iris %>% relocate(plant = Species, 
                  petal_w = Petal.Width, 
                  sepal_w = Sepal.Width)
```


### Recoding
Use `dplyr::recode` to change old values (first) into new values (second): 
```{r}
#| eval: false

df %>%
  mutate(sen_status = recode(
    sen_status,
    "N" = "NoSEN",
    "K" = "SENSupport",
    "E" = "EHCP",
    "S" = "Statement"))
```

Note that for some reason if you are recoding numeric to character, you need to place integers in quotes:
```{r}
df %>%
  mutate(fsm_eligibility = recode(fsm_eligibility,
                                  "0" = "NoFSM",
                                  "1" = "FSM"))
```

For reference, this is the way to recode in Base R:
```{r}
df$var[df$var == "Old Value"] <- "New Value"
```


Recipe: recode `gender` values to numeric and assign `NA_real_` if cannot be recoded (NA type must match the type of the other values).
```{r}
#| eval: true
starwars %>%
  select(name:mass, gender) %>%
  mutate(gender2 = recode(
    gender,
    "masculine" = 1,
    "feminine" = 2,
    .default = NA_real_
  )) %>% slice(78:83)
```

```{r}
# another example
df %>% mutate(
  maladmin_check = recode(
    maladmin_check,
    "0" = "code1",
    "1" = "code2",
    "2" = "error1",
    "3" = "error2"
  )) 
```


### Refactoring / relevelling
Use `fct_relevel` to reorder the level of factors. This can be useful for tabulation or graphing.
```{r}
# relevel a single variable
df %>%
  mutate(var1 = fct_relevel(var1,
                            c("f1", "f3", "f4", "f2")))

# relevel across several columns
df %>% 
  mutate(across(c(var1, var2),
                ~ fct_relevel(., c("High attainers", 
                                   "Standard & strong passes", 
                                   "Low attainers"))))
```


## Joining
There are two types of joins:

- Inner joins: retaining only rows that appear in BOTH tables (`inner_join`)
- Outer joins: retaining only rows from ONE of the two tables (`left_join`, `right_join`, `full_join`)

A left join will retain all rows in the first table (`df` below). Note that when your join contains duplicate column names, these will become "var1.x" and "var1.y" by default. To override this and avoid having to rename separately later, you can specify the suffixes you would like using the `suffix` argument.
```{r}
# when the primary key is the same
df1 %>% left_join(df2, by = "primaryKey", suffix = c("_t1", "_t2"))

# when primary keys are different
df1 %>% left_join(df2, by = c("primaryKey1" = "Primarykey2"))

# to only retain certain columns from the second dataframe:
df1 %>% left_join(select(df2, col1, col2), by = "primaryKey")
```

### Joining tips and extras
`suffix` - If you are joining tables with identical table names, these will be renamed `.x` for table1 and `.y` for table2. You can control the renaming:
```{r}
df1 %>% left_join(df2, by = "primaryKey",
                  suffix = c(".t1", ".t2"))
```

`multiple` - If you are joining wide and long-format data, there will be multiple matches in the latter. You can control what happens in this situation (but the default value of `all` is usually sufficient).
```{r}
df1 %>% left_join(df2, by = "primaryKey",
                  multiple = "all")
# all - returns every match detected
# first - returns first match in table2
# last - returns last match in table2
```

`unmatched` - By default, unmatched rows in outer joins will be dropped, but this can be controlled.
```{r}
df1 %>% left_join(df2, by = "primaryKey",
                  unmatched = "drop")

# drop - delete unmatched rows (default)
# error - throw an error
```

`relationship` - Explicitly tell R what kind of relationship to expect:

- "one-to-one"
- "one-to-many" 
- "many-to-one"
- "many-to-many" (doesn't provide any checks but prevents warnings)

A recipe for renaming columns in a big join: say you're joining two dataframes from different time points that have the same name columns. You want to keep both sets, but append something to the first set of names to keep it unique for later joins. 
```{r}
# first create a vector of column names to rename
cols_to_rename = c("URN", "CensusTerm", "NCYearActual",  "EthnicGroupMajor", "FSMeligible")

# append _y1 to distinguish from _y2 data joined later using rename_with
# (can also provide a suffix argument just in case names are duplicated)
df_joined = df1 %>%
  left_join(df2, by = "PMA",
            suffix = c("_cenY1", "_phonY1")) %>%
  rename_with(~paste0(., "_y1"), .cols = all_of(cols_to_rename))
```

Here, `rename_with` says: for all column names in `cols_to_rename`, paste the original name plus `_y1` at the end.

## Splitting a list of dataframes
```{r}
# split a dataframe according to a variable
form_list = split(MTC2022_kmt, MTC2022_kmt$FormID)

# turn each list element into a dataframe
form_list = lapply(seq_along(form_list), function(x) as.data.table(form_list[[x]]))

# extract each element into the global environment
lapply(seq_along(form_list), function(x) {
  assign(c("Form1_clean", "Form2_clean", "Form3_clean", "Form4_clean", "Form5_clean")[x], 
         form_list[[x]], envir=.GlobalEnv)})

# compute summary stats for each list element
lapply(form_list, function(x)
  x %>% rowwise() %>% mutate(formmark = sum(c_across(where(is.numeric)))) %>% 
    group_by(inputtype) %>% 
    summarise(n = n(),
              form_min = min(formmark),
              form_mean = mean(formmark),
              form_max = max(formmark)))
```

# Selecting
Use `dplyr::select()` to select columns by name or index. If you want to select column names that match a list, try this:
```{r}
col_list = c("name", "height", "mass")

starwars_subset = subset(starwars, select = names(starwars) %in% col_list)
head(starwars_subset)
```



# Working with strings

## Detect strings
`str_detect`
```{r}
# Recipe: use filter to find strings containing "white" in the skin_color col
starwars %>% 
  filter(str_detect(skin_color, "white")) %>%
  select(1:5)

# search for multiple patterns in a string
df %>% filter(str_detect(cheating_check, "reduced|approp|missing"))

# use for summing
sum(str_detect(cheating_tab$cheating_check, "reduced|populated") > 0)

# look for where Review contains A to Z and a number between 0 and 2. 
df %>% mutate(review_check = case_when(
  str_detect(Review, "[a-zA-Z][0-2]") & (mark_change + outcome_change) > 0 ~ "Error"))
```


## Replace strings
`str_replace_all` (this replaces all matches - `str_replace` only replaces the first match)
```{r}
# Recipe: replace "[1]" with a longer description, e.g. "1 audible"
df = df %>%
  mutate(var = str_replace_all(var,
  c("[1]" = "1 audible",
    "[2]" = "2 audio",
    "[3]" = "3 colour"))
  )
```

## Tidy up strings
`str_to_lower`, `str_to_title`
```{r}
starwars %>% mutate(name = str_to_lower(name))

starwars %>% mutate(gender = str_to_title(gender))

 # to do this across multiple columns
starwars %>% mutate(across(c(name, homeworld, species), ~ tolower(.x)))
```

`str_length`
```{r}
#| eval: true
string = "This is a string."
str_length(string)
```

`str_trim` removes whitespace from either or both sides
```{r}
#| eval: true
string1 = "    This is a string with padding           "
str_trim(string1, side = c("both"))
```

`str_squish` removes whitespace from both sides and replaces internal whitespace with a single space.
```{r}
#| eval: true
string2 = " This is a string with padding and a full stop     ."
str_squish(string2)
```

`str_trunc` truncates a string to the desired length. Note: by default, it will provide ellipsis.
```{r}
#| eval: true
long_string = "The Sleepwalkers: How Europe Went to War in 1914"

# default with ellipsis (note that ellipsis starts AFTER character 19)
str_trunc(long_string, 19)

# turn off ellipsis
str_trunc(long_string, 19, ellipsis = "")
```


## Subset strings
`str_sub`
```{r}
#| eval: true
string = "I love R"
str_sub(string, 3, 6)
str_sub(string, 1, -2) # remove final character 
str_sub(string, 2, -1) # remove first character (-1 means end of string)
```

```{r}
# can be used as part of a case_when, e.g. check that the first element of TotalMarkA is 
# NOT "E" or "%"
case_when(
  !str_sub(TotalMarkA, 1, 1) %in% c("E", "%")
)
```

Extract the first word in a string
```{r}
df %>% mutate(Award_body = sapply(strsplit(QUIDTITLE, " "), function(x) x[1]))
# e.g. this takes "TCL Level 1 Award In..." and extracts "TCL". 
```


## Regular expressions
Identify digits, letters, or a combination
```{r}
# Recipe: find names containing digits
starwars %>%
  filter(str_detect(name, "[:digit:]")) %>% select(1:5)
```

Also try:

- `"[:alnum:]"` - letters and numbers
- `"[:alpha:]"` - letters
- `"[:punct:]"` - any punctuation
- `"[:graph:]"` - letters, numbers, and punctuation
- `"[:space:]"` - space characters

```{r}
# look for strings containing A-Z, a-z, and a number between 2 and 7
str_detect(string, "[a-zA-Z][2-7]")

# look for strings starting with A-Z, a-z, 0-9, dashes, apostrophes, or parentheses and ending
# with an asterisk
str_detect(string, "^[A-Za-z0-9\\s\\-\\'\\(\\)]*$")

# find strings with 5 consecutive digits
str_detect(string, "\\d{5}")

# extract chunks of 5 numbers in strings
str_extract(string, "[0-9]{5}")

# find and extract the first whole word
str_extract("John Smith", "\\w+") # gives: "John"
```

Anchors
```{r}
# starts with a
str_detect(string, "^a")

# ends with a
str_detect(string, "a$")
```

Some base R recipes
```{r}
# remove a final full stop (. needs to be escaped with \\ as it's a special character)
cleaned_vector = gsub("\\.$", "", vector)

# Insert space (\\1) every 3 characters ({3}):
sub("\\s+$", "", gsub('(.{3})', '\\1 ', vector))

# retain only numbers
data = c("foo_5", "bar_7")
gsub(".*_", "", data)
readr::parse_number(data)

# retain any number at the end of a string, e.g. GR3 -> 3
df %>% mutate(Grade = str_extract(QUALTYPE, "\\d+$"))

# General syntax for grep()
grep(pattern_to_find, data, ignore.case = TRUE, value = T) # value=T: show results

# Filter cells that contain the letter F:
data %>% filter(grepl("F", variable))

# look for string consisting of letter-letter-number-number-letter-letter
grep("[A-Z][A-Z][0-9][0-9][A-Z][A-Z]", df$var, value=T)

# find a string beginning with a letter between A-Z, followed by a 2, followed by x
grep("[a-z]2[x]", data, value = T)

# Extract everything between parentheses
df$var = gsub("[\\(\\)]", "", regmatches(df$var, gregexpr("\\(.*?\\)", df$var)))
```

Some revived code
```{r}
#| eval: true
# Quantifiers
quantifiers <- c("1","10","100", "1000","10000", "200")

# Is a 1 followed by 0 or more 0s?
quantifiers[str_detect(quantifiers, "10*")]
# is a 1 followed by 1 or more 0s?
quantifiers[str_detect(quantifiers, "10+")]


# find 'pupil' OR 'pupils'
optional_element <- c("pupil", "pupils", "students")
#find pupil or it's plurals
optional_element[str_detect(optional_element, "^pupils?$")]

# Quantifiers and wildcards
# Now with groupings and anchors
#1 with even 0s
quantifiers[str_detect(quantifiers, "^1(00)*$")]
# This returns one hundred and one thousand (find 1 followed by 2 or 3 zeros)
quantifiers[str_detect(quantifiers, "^10{2,3}$")]
# anything followed by 2 zeros
quantifiers[str_detect(quantifiers, "^.0{2}$")]


# character classes
character_class <- c("hat school", "mat school", "sat school"
                     ,"cat school")

# first character must be h or s
character_class[str_detect(character_class, "[hs]at school")]
# first character must NOT be h or s
character_class[str_detect(character_class, "[^hs]at school")]


# conditionals
conditionals <- c("mixed", "boys", "girls")
# detect mixed or boys
conditionals[str_detect(conditionals, "^(mixed|boys)$")]

escapes <- c("school", "schools", "school's")
# find possessive schools
escapes[str_detect(escapes, "school\\'s")]

# ERE Character classes
character_classes_ERE <- c("West School", "1 School", 
                           "North School")
# find a school with a digit in it
character_classes_ERE[str_detect(character_classes_ERE, "[:digit:]")]
```


# Generating and sampling random data
## Generate strings
```{r}
# create 25 strings of 6 characters each, using A-Z and 0-9 randomly
library(stringi)
code = stringi::stri_rand_strings(25, 6, pattern = "[A-Z0-9]") 

# Add prefix 'OLLIH-' onto the random strings, e.g. OLLIH-273
codes = paste0("OLLIH-", stringi::stri_rand_strings(25, 3, pattern = "[0-9]"))
```

## Generate numbers
```{r}
# replicate numbers 1 to 10, 10 times (1,2,3,4... 1,2,3,4..., 1,2,3,4...)
rep(c(1:4), times = 10) 

# Replicate numbers a certain number of times each (1, 1, 2, 2, 3, 3, 4, 4, ...)
rep(c(1:4), times = 5, each = 2)

# set up random number generator to get replicable results
set.seed(1)

# Generate 5 random digits between 0 and 1
runif(5) 

#  With a specified range
runif(5, min = 0, max = 100) 

# Alternative method using sample()
sample(x = 1:100, size = 3, replace = F)

# Random normal distribution  
rnorm(4, mean = 50, sd = 10)

# Truncated normal distribution
library(msm)
rtnorm(n, mean=0, sd=1, lower=-Inf, upper=Inf)
```


# Dates and times
Convert character to date
```{r}
#| eval: true
my_date = "08/10/2021"
as.Date(my_date, "%d/%m/%Y")
```

Often when you read in Excel data the dates get messed up. Usually these will get converted to character: one good solution is to convert them to numeric first, and then to a date:
```{r}
#| eval: false

df %>%
  mutate(date_col = as.numeric(date_col)) %>%
  mutate(date_col = lubridate::as_date(date_col, 
                                       origin = "1899-12-30",
                                       format = "%d-%m-%Y")
```


There are two ways to extract the year from a `date` object:
```{r}
#| eval: true
my_date = as.Date("08/10/2021", "%d/%m/%Y")

year = format(as.Date(my_date, format="%d/%m/%Y"),"%Y")
year
```

The method above results in character type, e.g. "2021". It may be better to use `lubridate` to extract the year as this is simpler and results in a double type which is more compatible with graphing methods:
```{r}
#| eval: true
my_date = as.Date("08/10/2021", "%d/%m/%Y")
my_date
my_year_dbl = lubridate::year(my_date)
my_year_dbl
```

Calculate the difference between two date/time objects in years
```{r}
df %>%
  mutate(age = as.numeric(difftime(date2, date1, units = "weeks") / 52.25))

# a more accurate method is time_length as this takes into account the fact that months and years
# dont have the same number of days
df %>%
  mutate(age = lubridate::time_length(difftime(date2, date1), "years"))
```


Filtering dates
```{r}
filter(date > as.Date("2001-01-19") & date < as.Date("2003-03-21")) 
```


# Summarising
`get_summary_stats` from the `rstatix` package provides different `type`s of summary statistics:

* full
* common
* robust (same as median_iqr)
* mean_sd
* mean_ci
* median_iqr
* median_mad
```{r}
#| eval: true
mtcars %>%
  group_by(cyl) %>%
  get_summary_stats(c(mpg, wt), 
                    type = "mean_sd")
```


## Crosstabulation
`janitor::tabyl`

```{r}
#| eval: true
# one-way tabyl

starwars %>% 
  mutate(eye_color = na_if(eye_color, "unknown")) %>%
  tabyl(eye_color, show_na = T) %>%
  adorn_pct_formatting(digits=2) %>%
  arrange(desc(n)) %>%
  adorn_totals() 
```

Tip: for a two-way tabyl, remove double whitespaces by including a call to `str_replace_all`
```{r}
#| eval: true
# two-way tabyl

ggplot2::diamonds %>%
  tabyl(cut, color, show_na = T) %>%
  adorn_totals(c("row", "col")) %>%
  adorn_percentages("row") %>%
  adorn_pct_formatting()  %>%
  adorn_ns(position = "front") %>% # use "rear" to switch position
  mutate(across(everything(), ~ str_replace_all(., "  ", " ")))
```


# Iteration/for loops
```{r}
#| eval: true
data = data.frame(
  df1 = rnorm(10),
  df2 = rnorm(10),
  df3 = rnorm(10))

col_median = function(df){
  output = numeric(ncol(df))
  for (i in seq_along(df)){
    output[[i]] = median(df[[i]])
  }
  output
}
col_median(data)
```

Use a for loop to create a list of dataframes with random data
```{r}
set.seed(2020)

# create empty list
e <- list()

for (i in 1:3) {
  e[i] <- list(rnorm(n = 30, mean = 50, sd = 12))
  samples = data.frame(e)
}
```



# Functions
Some examples of functions and nifty tricks for making things dynamic.

Basic structure of a function. Note that default values such as NA can be expressed to show that an argument is optional. The ... notation indicates multiple arguments. 
```{r}
my_func = function(arg1, arg2 = NA, ...){
  # some operation
}
```


## Error handling using `stop`
```{r}
my_func = function(x, y) {
  
  if (!exists("folder_file_path"))
    stop("Error: Please specify folder file path for datafeed")
}


my_func = function(df, subject){
  # subject argument check
  acceptable_subjects = c("G", "M")
  
  if (!subject %in% acceptable_subjects)
    stop("Error: Please choose 'M' or 'G' as subject.")
}
```


## Extract arguments using `substitute` and `deparse`
```{r}
my_func = function(datafile){
  df_name = deparse(substitute(datafile))
  
  # for example, you might then use df_name later:
  df %>% select(!!paste("KS2", df_name, "_CGS", sep=""))
}

# Example: extract a variable name to use as a ggtitle:

char_graph = function(df, outcome, char){
  
  # extract text contained in 'char'
  title = deparse(substitute(char))
  
  # create graph and invoke title object in ggtitle
  df %>%
    ggplot(aes(x = {{outcome}}, y = prop, fill = {{char}})) +
    ggtitle(title)
    ...
}}
```


## Exporting to the global environment 
Using `<--`
```{r}
my_func = function(x, y){
  prod <<- x * y
}
```

Using `assign` for dynamic naming
```{r}
my_func = function(x, y){
  ...
  assign(paste0("dca_rows_", df_name, "_flagged"), # dynamic name
         dca_rows_flagged, # object to export
         envir = .GlobalEnv)
}
```

Remove something from the global environment
```{r}
if (nrow(df_flagged)==0){
    rm(df_flagged, pos = ".GlobalEnv")
  }
```

Example of an if statement based on logical argument in function all
```{r}
produce_desc_stats_t <- function(data, strata_input, exclude_nonpartner_schools){
     data %>%
     filter(case_when(exclude_nonpartner_schools == TRUE ~ partner_w12345 != "Non-partner school")) %>% 
     {if exclude_nonpartner_schools == TRUE filter(partner_w12345 != "Non-partner school")} %>% 
     select(...)
```


## Tidy evaluation
Tidy evaluation is a way of achieving data masking, i.e. changing the context of computation. Whenever we use e.g. `starwars %>% filter(height > 200)`, we are using data masking (i.e. `height` instead of `starwars$height`). To change the context of computation, we need to suspend evaluation ('quoting') before resuming in a different environment. Tidy eval is only necessary when arguments vary, so is only really useful within functions.

We use a quote and unquote pattern called interpolation. For singular arguments we use `enquo` to quote and `!!` to unquote. 
```{r}
#| eval: true
grouped_mean = function(data, group_var, sum_var){
  group_var = enquo(group_var)
  sum_var = enquo(sum_var)
  
  data %>%
    group_by(!!group_var) %>%
    summarise(mean = mean(!!sum_var))
}

grouped_mean(mtcars, am, hp)
```
For multiple arguments, use `enquos` and `!!!`. Here, multiple arguments are indicated by `...`. 
```{r}
#| eval: true
grouped_mean_multiple = function(.data, .sum_var, ...){
  .group_vars = enquos(...)
  .sum_var = enquo(.sum_var)
  
  .data %>%
    group_by(!!!.group_vars) %>%
    summarise(mean = mean(!!.sum_var))
}

grouped_mean_multiple(mtcars, hp, am, gear)
```

Don't forget `get()` and `{{}}`. Here is an example where you might want to produce many tables with different variables:
```{r}
#| eval: false

char_table = function(df, outcome, char){
  df %>% 
    filter(!is.na({{char}})) %>%
    tabyl({{outcome}}, {{char}}) %>% 
    adorn_totals(c("row", "col")) %>%
    adorn_percentages("row") %>%
    adorn_pct_formatting()  %>%
    adorn_ns(position = "rear") %>%
    adorn_title()
}

char_table(my_dataframe, pass_rate, ethnicity)
```


Here, use `if_any` and `coalesce` to take the first non-missing value within a pre-specified list, and then derive a new variable based on a dynamic name (`{{varname}}`)
```{r}
my_func = function(df, subject){
  varname = paste0("TotalMarkA_", subject)
  
  tab <<- df %>%
      mutate(in_hierarchy = if_any(c(TestMark1, TestMark2), ~ . %in% hierarchy)) %>%
      mutate(value = ifelse(in_hierarchy == TRUE, coalesce(TestMark1, TestMark2), NA)) %>%
      mutate({{varname}} := case_when(
        value == TotalMarkA & Subject == subject ~ "OK",
        Subject != subject ~ "N/A: diff subj",
        TRUE ~ paste0("Error: ", varname, " incorrect"))) 
}
```



## Nifty functions for various tasks
### Compare NA
```{r}
# function for comparing NAs (i.e. because if two values are NA, you can't 
# compare them for equivalence)
  compareNA <- function(v1,v2) {
    same <- (v1 == v2) | (is.na(v1) & is.na(v2))
    same[is.na(same)] <- FALSE
    return(same)
  }

# example: if TA and TestStatus are different, assign 3
!compareNA(TA, TestStatus)) ~ 3


# To convert factor to numeric. If you just use as.numeric, this will use the factor levels, not the
# "number" that you see in the vector.
df %>% mutate(my_fac = as.numeric(levels(my_fac)[my_fac])
```

# Text analysis
```{r}
# tidytext package

```



# Presentation
## `gt` package
This package provides some options for high-quality, highly customisable table outputs. 
```{r}
#| eval: true
# dummy data
df = gt::countrypops %>%
  select(-c(starts_with("country_code"))) %>%
  filter(str_detect(country_name, "Republic")) %>%
  filter(year %in% c(2000, 2010, 2020)) %>%
  pivot_wider(names_from = year, values_from = population)

# simple table
df %>%
  gt(rowname_col = "country_name") %>%
  tab_spanner(label = "Total population", columns = everything()) %>%
  fmt_integer() %>%
  cols_align("left") %>% 
  tab_row_group(label = "group 1", rows = 1:2) %>%
  tab_row_group(label = "group 2", rows = 3:4) %>%
  row_group_order(c("group 1", "group 2")) %>%
  tab_options(table.align='left',
              table.font.size = 14,
              table.font.names = "Arial") %>%
  tab_header(title = "Population table") %>%
  tab_options(table.align='left') %>% 
  tab_style(
    style = list(cell_text(weight = "bold")),
    locations = cells_body(
      columns = c(`2020`),
      rows = c(2,3))) %>%
  opt_stylize(style = 3) %>%  # styles 1-6
  tab_footnote("Source: World Bank")
```

### Other `gt` functions
```{r}
# to wrap column or header names when they're too long - use md() and <br/>
gt() %>% 
  tab_header(title = md("Proportion of 16-18 learners <br/> by highest study aim, 2020/21"))

# custom column names
gt() %>% cols_label(old_name = "New name")

# do the same with cols_label:
gt() %>% cols_label(var1 = md("Long variable name <br/> that needs splitting"))

# format number for specific columns with separator mark and decimals
gt() %>% fmt_number(
  columns = c('n.16', "n.17", "n"),
  sep_mark = ',', decimals = 0) 

# format percent on certain cols and rows
gt() %>%
  fmt_percent(columns = c(2,3), 
             rows = c(2:6),
             decimals = 2)

# add conditional formatting
gt() %>%
  data_color(columns = 2:4, 
             direction = "column",
             palette = colorRampPalette(c("lightblue", "darkblue"))(100))

# save a gt table object (assuming it's called 'tab')
gtsave(tab, "tab.rtf") # for word format
gtsave(tab, "tab.png")
```


### Conditional formatting
```{r}
#| eval: true
#| echo: false

# fake data for below
set.seed(2024)
df = data.frame(
  var_a = round(rnorm(10, 10, 2),0),
  var_b = round(runif(10, 5, 15),0),
  var_c = round(rnorm(10, 300, 50),0),
  var_d = c("true", "false", "true", "true", "true", "false", "false", "true", "false", "false")
)
```


```{r}
#| eval: true

df %>%
  gt() %>%
  # example 1 - use a readymade palette from RColorBrewer
  # note that you can also specify columns = everything() or columns = is.numeric,
  data_color( 
    columns = c(var_a), 
    rows = everything(), # or try 1:16 for row indices
    direction = "row",
    colors = scales::col_numeric( 
      palette = "PuOr",   # or: c("blue", "red")
      domain = c(0, 15)), # leave NULL if you want to use the full range of data
    apply_to = "fill"
    ) %>%
  # example 2 - use a custom palette
  data_color( 
    columns = c(var_b), 
    colors = scales::col_numeric( 
      palette = c('white', 'orange', 'red'),
      domain = c(0, 15)),
    ) %>%
  # example 3 - define colour based on if condition
  data_color(
    columns = var_c, 
    rows = var_c < 300,
    method = "numeric",
    palette = c("red")
  ) %>%
  # example 4 - colour based on text value
  data_color(
    columns = var_d,
    colors = scales::col_factor(
      palette = c("green", "red"),
      domain = c("false", "true")
    )
  )
# notes: can change apply_to to 'text' or 'fill (default)';
# if a cell is NA for formatting, it will be grey (change with
# na_color). 
```

## `gtsummary`
`gtsummary` builds on `gt` and is used to summarise dataframes, regession models, and more. Its main function is `tbl_summary`. 
```{r}
#| eval: true 

mpg %>%
  select(displ, cty, hwy, drv, class) %>%
  gtsummary::tbl_summary(
    by = class,
    statistic = list(
      all_continuous() ~ "{mean} ({sd})",
      all_categorical() ~ "{n} / {N} ({p}%)"   # can just have {n} if preferred
    ),
    digits = all_continuous() ~ 2,
    missing = "no" # whether to include missing values in separate rows
  ) %>% 
  modify_caption("<div style='text-align: left; font-weight: bold;'> Table 1</div>")

# not sure if you actually need %>% as_gt()


# add p-values for comparisons
mpg %>%
  filter(class %in% c("compact", "pickup")) %>%
  select(displ, cty, hwy, class) %>%
  gtsummary::tbl_summary(
    by = class,
    statistic = list(all_continuous() ~ "{mean} ({sd})"),
    digits = all_continuous() ~ 2
  ) %>%
  add_p()

# add CI
mpg %>%
  filter(class %in% c("compact", "pickup")) %>%
  select(displ, cty, class) %>%
  gtsummary::tbl_summary(by = class,
              type = all_continuous() ~ "continuous2",
              statistic = list(
                all_continuous() ~ c(
                  "{mean} ({sd})",
                  "{min}-{max}")),
              digits = all_continuous() ~ 2) %>% 
  add_p() %>%
  add_ci(pattern = "{stat} ({ci})")
```

Saving a `gtsummary` object as html or docx:
```{r}
table %>%
  gtsummary::as_flex_table() %>% 
  flextable::save_as_html(path = "./filename.html")
  # or  flextable::save_as_docx(path = "./filename.docx")
```



## Quarto documents
Below is a template for a Quarto YAML. Note: include `standalone` and `embed-resources` if you want to be able to share or view the HTML file on another device. 
```{r}
---
title: "My Title"
date: last-modified
date-format: "[Last updated:] DD/MM/YYYY HH:mm"
format:
  html:
    standalone: TRUE
    embed-resources: TRUE
    toc: true
    toc-depth: 3
    toc-expand: 1
    number-sections: FALSE
    toc-location: left
    df-print: kable
    code-fold: TRUE
    theme: default
    fontsize: 1em
    link-external-newwindow: TRUE
execute:
  warning: FALSE
  echo: TRUE
  eval: TRUE
---
```

Headings and subheadings are specified by prefixing hashtags - `#` for level 1, `##` for level 2, and so on. To cross-reference another section in your Quarto document, add a label for each section in curly brackets:

`# Packages {#sec-packages}`

To reference this use an at-symbol as a link:
`See @sec-packages` - produces: See @sec-packages. (in an html Quarto output if you hover over this link it should display a preview of that section in the document)

### URLs
```{r}
See the following [page](www.insert-url-here.com). 
```

### Figure options
```{r}
# Add the following at the beginning of a code chunk:
# (note that figures are centre-aligned by default)
#| fig-align: "left"
#| fig-height: 4
#| fig-width: 4
```

Running Quarto or Markdown render does not save the objects in the workspace. You can use code to run an .Rmd or .qmd file which will save the objects. The code below needs to be copied and pasted into the console. 
```{r}
# for markdown
rmarkdown::render(input = "MTC_4a_model_key_mouse.Rmd", 
                  output_file = paste0("filename-", Sys.Date(), ".html"),
                  output_dir = "./Outputs")

# for quarto (note that there is no option to specify the output directory here)
quarto_render(input = "notebook.qmd", 
              output_format = "html",
              output_file = paste0("filename-", Sys.Date(), ".html"))
```


# Create Excel workbook
Use the `openxlsx` package to export various objects, dataframes, or outputs to an Excel file with multiple worksheets.
```{r}
library(openxlsx)

# in this case, only export objects if they have been created in the environment
# (otherwise just remove the if statements)

qa_workbook = createWorkbook("QA_workbook")

if(exists("allowable_pupil_flagged")){
  addWorksheet(qa_workbook, "PupilValuesErr")
  writeDataTable(qa_workbook, sheet = "PupilValuesErr", allowable_pupil_flagged, startRow=1,
                 tableStyle = "none")
}

if(exists("upn_invalid_flagged")){
  addWorksheet(qa_workbook, "InvalidUPN")
  writeDataTable(qa_workbook, sheet = "InvalidUPN", upn_invalid_flagged, startRow=1,
                 tableStyle = "none")
}

saveWorkbook(qa_workbook, file = paste0("./Outputs/", "KS2DF_Queries_", date, "_", seq, "_R",  ".xlsx"), 
             overwrite = TRUE)
```


Another more complicated example:
```{r}
xl_output = function(form = NULL,
                     model = NULL, 
                     mod_fit = NULL,
                     item_fit_ref = NULL, 
                     item_fit_focal = NULL,
                     item_infit_ref = NULL,
                     item_infit_foc = NULL,
                     resid_ref_df = NULL,
                     resid_focal_df = NULL,
                     ref_cases = NULL, 
                     focal_cases = NULL,
                     DIFs = NULL, 
                     DTF = NULL, 
                     TCC = NULL, 
                     sDTF = NULL){
  # define styles
  posStyle <- createStyle(fontColour = "#006100", bgFill = "#C6EFCE")
  negStyle <- createStyle(fontColour = "#9C0006", bgFill = "#FFC7CE")
  hs <- createStyle(fontColour = "#ffffff", fgFill = "#4F80BD", halign = "center",
                    valign = "center")
  colStyle <- createStyle(halign = "center")
  center_and_dp = createStyle(halign = "center", numFmt = "#.000")
  hs = createStyle(halign = "center")
  
  # create workbook
  workbook = createWorkbook(paste("MTC2022", form))
  
  # README ---
  addWorksheet(workbook, "readme")
  
  writeData(workbook, sheet = "readme", startCol = 1, startRow = 2, "This spreadsheet presents the results of IRT modelling to examine the potential bias introduced by different input methods in the 2022 MTC.")
  writeData(workbook, sheet = "readme", startCol = 1, startRow = 3, "It contrasts users who use different input methods in multiple group IRT models for the purpose of analysing differential item and test functioning.")
  
  writeData(workbook, sheet = "readme", startCol = 1, startRow = 5, paste("Analysis here is based on ", form, " with ", model@Data[["N"]], " respondents", sep=""))
  
  writeData(workbook, sheet = "readme", startCol = 1, startRow = 7, paste("File created:", Sys.time()))
  writeData(workbook, sheet = "readme", startCol = 1, startRow = 8, paste("By user:", Sys.getenv("USERNAME")))
    writeData(workbook, sheet = "readme", startCol = 1, startRow = 9, paste("Using", R.version[["version.string"]]))

  writeDataTable(workbook, sheet = "readme", session_package_info, startRow=17, withFilter = FALSE, tableName = "R_packages_used") 
  
  
  # MODEL FIT ---
  addWorksheet(workbook, "model_fit")
  writeDataTable(workbook, sheet = "model_fit", mod_fit, startRow=2, withFilter = FALSE, headerStyle = hs,
                 tableName = "Model_fit")
  
  setColWidths(workbook, "model_fit", cols=2:26, widths = 12)
  
  # add notes
  writeData(workbook, sheet = "model_fit", paste("MTC2022", form, "model fit"), startCol = 1, startRow = 1)
  
  # more notes after table
  writeData(workbook, sheet = "model_fit", startCol = 1, startRow = 5, "Notes:")
  writeData(workbook, sheet = "model_fit", startCol = 1, startRow = 6, "M2 = Maydeu-Olivares & Joe (2006) limited information test procedure")
  writeData(workbook, sheet = "model_fit", startCol = 1, startRow = 7, "P-value of M2 should be >= 0.05 but large N makes it overly sensitive (de Ayala, 2022 p121)")
  
  # conditional formatting to highlight which indices show good fit
  conditionalFormatting(workbook, sheet="model_fit", cols=3, rows=3, rule=" < 0.05 ", type = "expression",style = negStyle)
  conditionalFormatting(workbook, sheet="model_fit", cols=4:6, rows=3, rule=" < 0.06 ", type = "expression",style = posStyle)
  
  # model info
  writeData(workbook, sheet = "model_fit", startCol = 1, startRow = 14, "MIRT model call:")
  writeData(workbook, sheet = "model_fit", startCol = 1, startRow = 15, deparse(model@Call))
  
  writeData(workbook, sheet = "model_fit", startCol = 1, startRow = 20, paste("N: ", model@Data[["N"]]))
  writeData(workbook, sheet = "model_fit", startCol = 1, startRow = 21, paste("Iterations: ", model@OptimInfo[["iter"]]))
  writeData(workbook, sheet = "model_fit", startCol = 1, startRow = 22, paste("Converged: ", model@OptimInfo[["converged"]]))
  
  addStyle(workbook, sheet = "model_fit", style = center_and_dp, rows = 3, cols = 3:10, gridExpand=T)
  
  # ...
  # apply conditional formatting for misfit col:
  conditionalFormatting(workbook, sheet="item_fit", cols=6, rows=33:57, type = "contains", rule = "misfit", style = negStyle)
  conditionalFormatting(workbook, sheet="item_fit", cols=13, rows=33:57, type = "contains", rule = "misfit", style = negStyle)
  
  # change to number formatting (to avoid scientific notation in Excel)
  addStyle(workbook, sheet = "item_fit", style = center_and_dp, rows = 3:27, cols = 5)
  addStyle(workbook, sheet = "item_fit", style = center_and_dp, rows = 3:27, cols = 12)
  
  #center
  addStyle(workbook, sheet = "item_fit", colStyle, rows = 3:28, cols = 2:4, gridExpand = TRUE)
  
  # ...
  # FOCAL (MOUSE)
  writeData(workbook, sheet = "residuals", startCol = 1, startRow = 32, paste("Q3 residuals for", foc, "group"))
  writeData(workbook, sheet = "residuals", startCol = 1, startRow = 33, paste("Cases to check:", focal_cases))
  writeDataTable(workbook, sheet = "residuals", resid_focal_df, startRow=34, startCol = 1, withFilter = FALSE,
                 headerStyle = hs, rowNames=T, tableName = "Q3_Residuals_mouse")
  #styling
  addStyle(workbook, sheet = "residuals", center_and_dp, rows = 6:30, cols = 2:26, gridExpand = TRUE)
  addStyle(workbook, sheet = "residuals", center_and_dp, rows = 35:60, cols = 2:26, gridExpand = TRUE)
  
  
  # conditional formatting to identify problematic cases
  posStyle <- createStyle(fontColour = "#006100", bgFill = "#C6EFCE")
  conditionalFormatting(workbook, sheet="residuals", cols=2:25, rows=6:30, rule=" > 0.2236", type = "expression",style = posStyle)
  conditionalFormatting(workbook, sheet="residuals", cols=2:25, rows=35:60, rule=" > 0.2236", type = "expression",style = posStyle)
  
  # set col widths
  setColWidths(workbook, "residuals", cols=2:26, widths = 5.00)
  # ...
  
  # DTF ---
  addWorksheet(workbook, "DTF")
  # call external function dtf_converter() to split DTF list into 3 dataframes (obs, ci, p)
  dtf_converter(DTF)
  
  writeData(workbook, sheet = "DTF", startCol = 1, startRow = 1, "Differential test functioning")
  writeData(workbook, sheet = "DTF", startCol = 1, startRow = 2, paste("sDTF = signed differential test functioning (positive value indicates bias in favour of reference group ", "(", ref, ")", sep=""))
  writeData(workbook, sheet = "DTF", startCol = 1, startRow = 3, "uDTF = unsigned differential test functioning (absolute value of bias regardless of direction)")
  
  # obs
  writeDataTable(workbook, sheet = "DTF", obs, startRow=5, startCol = 1, withFilter = FALSE,
                 headerStyle = hs, rowNames=T, tableName = "DTF_obs")
  
  # ci
  writeDataTable(workbook, sheet = "DTF", ci, startRow=8, startCol = 1, withFilter = FALSE,
                 headerStyle = hs, rowNames=T, tableName = "DTF_ci")
  
  # p
  writeDataTable(workbook, sheet = "DTF", p, startRow=12, startCol = 1, withFilter = FALSE,
                 headerStyle = hs, rowNames=T, tableName = "DTF_p")
  
  writeData(workbook, sheet = "DTF", startCol = 3, startRow = 13, "Tests whether sDTF is significantly non-zero")
  setColWidths(workbook, "DTF", cols=1:5, widths = 15)
  
  # avoid scientific notation for p-value
  addStyle(workbook, sheet = "DTF", style = center_and_dp, rows = 13, cols = 2, gridExpand=T)
  
  TCC[["main"]] = paste(form, " TCC", " (", sDTF_stat, ")", sep="")
  print(TCC)
  
  insertPlot(workbook, sheet = "DTF", width=6, height=4, startRow=16, startCol=1, dpi = 300)
  
  # sDTF plot
  print(sDTF)
  insertPlot(workbook, sheet = "DTF", width=6, height=4, startRow=16, startCol=6, dpi = 300)
  
  # save
  saveWorkbook(workbook, file = paste("MTC2022", "_", ref, "_", foc, "_", form, ".xlsx", sep=""), overwrite = TRUE)
}
```


# Odds and ends
```{r}
gc() # garbage collection - A call of gc causes a garbage collection to take place.
# This will also take place automatically without user intervention, and the primary purpose
# of calling gc is for the report on memory usage. However, it can be useful to call gc after
# a large object has been processed or removed, as this may prompt R to return memory to the
# operating system. It may be more helpful to run this after running some sections of the code.

# use across and ifelse together
df %>% mutate(across(starts_with("y_"), ~ifelse(.x %in% c("EXS", "GDS"), 1, 0)))

df %>% summarise(across(starts_with(c("c_", "y_")), ~mean(.x, na.rm = T)))

# create a lagged variable
df %>% mutate(across(contains("_lag"), ~ dplyr::lag(.x, n = 1, default = NA)))

# recipe to create new column which counts whether the number of distinct values in a column is above 5
df %>% mutate(five_distinct = ifelse(n_distinct(upn) >=5, 1, 0))

# dplyr count
mpg %>% count(manufacturer, name = "count", sort = T)

# starwars %>% count(gender, eye_color, .drop = F)

```

## Encoding
```{r}
# example of how to deal with HTML character entity encoding, e.g. when you have things like &#211;rla (Orla)

# use xml2 to fix this:
df %>%
  mutate(given_name_s = xml2::xml_text(read_html(paste0("<p>", given_name_s, "</p>"))))
```


# Web scraping
```{r}
#| echo: false
library(rvest)

url <- "https://www.gov.uk/government/statistical-data-sets/monthly-management-information-ofsteds-school-inspections-outcomes"

pg <- read_html(url)

# NB there is no October 2019, or April and Sept 2020 MI data
monthly_files <- html_attr(html_nodes(pg, "a"), "href") %>% 
  as_tibble() %>%
  filter(
    (
      # Only csv files
      grepl("https://assets.publishing.service.gov.uk/", value) & grepl("Tables2|latest", value) & grepl(".csv", value) &
        rm_between(value, "https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/", "/", extract = TRUE) >= 838014
    ),
    value != "https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/920761/Management_information_-_state-funded_schools_1_September_2015_to_31_August_2019.csv"
  ) %>%
  distinct() %>% 
  pull(value)

# count the number of links
num_links <- length(monthly_files)

# Download files --

# Create function that downloads file with base file name to data folder
download.w.file.name <- function(url, folder){
  
  download.file(url, mode="wb", method = "libcurl", destfile = file.path(folder, basename(url)))
  
}

# Download
lapply(monthly_files, download.w.file.name, folder = ofsted_dir)

# count the number of files downloaded
ofsted_file_paths <- list.files(ofsted_dir, full.names = TRUE)

num_files <- length(list.files(ofsted_dir))

print(paste(num_files, "files downloaded from", num_links, "links"))
```


# Statistical tests

## Chi-square
Example: did the survival rate on the Titanic differ according to whether passengers were in first, second, or third class?
```{r}
#| eval: true
# extract data on titanic survival
titanic = titanic::titanic_train

titanic %>%
  tabyl(Survived, Pclass) %>%
  adorn_title()

# look at observed - save this table for later
results = table(titanic$Survived, titanic$Pclass)
```

Conduct the chi-square test on this table
```{r}
#| eval: true
options(scipen=999)

chisq.test(results)

# look at expected frequencies. All should be > 5, otherwise consider fisher.test
chisq.test(results)$expected 
```

Residuals
```{r}
#| eval: true
# positive residual indicates a positive correlation between the two variables; negative indicates negative. 
chisq.test(results)$residuals

# visualise residuals
# corrplot::corrplot(chisq.test(results)$residuals, is.cor = FALSE)
```

## Correlation
Use `cor()` to get correlation coefficients. Note the following options:

* `method` = c("pearson", "spearman", "kendall")
* `use` = c("everything", "pairwise", "pairwise.complete.obs")

```{r}
#| eval: true

# data
states = data.frame(state.x77)

# compute a correlation between two variables:
cor(states$Population, states$Murder, method = "pearson")

# compute a correlation matrix for all variables (must be numeric)
cor(states, use = "everything", method = "pearson")
```

```{r}
# Delete the upper triangle
state_cor = cor(states, use="everything", method="pearson")
state_cor[upper.tri(state_cor)] <- NA
```

Test correlation for significance
```{r}
#| eval: true
# for a single pair of variables
cor.test(states$Illiteracy, states$Murder, method = "pearson")

# for all variables
psych::corr.test(states, method = "pearson", use = "complete")
```

Correlation with `rstatix` 
Correlation tests for specified set of variables Optionally, filter out autocorrelations with `cor != 1` and create a new variable for the coefficient of determination (R2) where 0.01 is small, 0.09 to 0.25 is medium, and > 0.25 is a large effect size. 
```{r}
#| eval: true
states %>% rstatix::cor_test(vars = c(Population, Income, Illiteracy), 
                             method = "pearson", 
                             use = "complete.obs") %>%
  filter(cor != 1) %>%
  mutate(R2 = cor^2)
```

Correlation tests for all variables
```{r}
states %>% rstatix::cor_test(method = "pearson", use = "complete.obs")
```

### Graphing
```{r}
#| eval: true
corr <- round(cor(states), 3)
p.mat <- cor_pmat(corr)

ggcorrplot(corr, method="square", type="upper", 
           lab=T, lab_size = 3, tl.cex = 10, digits=4, p.mat= p.mat) +
  theme(legend.position = "none")
```


## T-test
Example: are SAT scores significant different according to gender? Conduct a Bartlett Test first to determine whether variances among the two groups are equal - this will determine which variant of the T-test to use.
```{r}
#| eval: true
bartlett.test(psych::sat.act$SATV ~ psych::sat.act$gender)
```

Alternatively, use a Wilcox (Mann-Whitney/U) test
```{r}
#| eval: true
# if p > .05, assume equal variances
wilcox.test(psych::sat.act$SATV ~ psych::sat.act$gender)
```

The main base R function is `t.test`. Note that `var.equal` controls whether a Student t-test or Welch t-test is conducted. A Welch test is a better alternative because it is less restrictive and doesn't assume equality of variances.

```{r}
#| eval: true
# Student T-test
t.test(psych::sat.act$SATV ~ psych::sat.act$gender, var.equal = T)
t.test(psych::sat.act$SATV ~ psych::sat.act$gender, var.equal = F)
```

To conduct t-tests among multiple variables simultaneously. Example: compare petal length, petal width, sepal length, and sepal width among the three plant species Setosa, Versicolor, and Virginica. Add a Benjimini-Hochberg correction for multiple comparisons and custom cutpoints for p-value asterisks. Note that `estimate1` and `estimate2` indicate the mean for each group, whereas `estimate` is the t-test coefficient itself. 
```{r}
#| eval: true
iris %>%
  pivot_longer(-c(Species)) %>%
  group_by(name) %>%
  t_test(value ~ Species, paired = F, detailed = T) %>%
  adjust_pvalue(method = "BH") %>%
  add_significance(cutpoints = c(0, 0.01, 0.05, 1),
                   symbols = c("**", "*", "")) %>%
  head()
```

## Effect size
Compute effect sizes to accompany the above t-test results (Cohen's D with Hedges correction):
```{r}
#| eval: true
iris %>%
  pivot_longer(-c(Species)) %>%
  group_by(name) %>%
  cohens_d(value ~ Species, paired = F, hedges.correction = T) %>%
  head()
```

## Regression
```{r}
#| eval: true

# model set up
# notes:
# - use scale(wt) to center variable
# - use I(weight^2) to add quadratic term

model = lm(mpg ~ wt * cyl + gear, data = mtcars)

# model output
summary(model)
```

Alternatively, use `broom` to get tidy model output
```{r}
#| eval: true

broom::glance(model) # gives r2, AIC/BIC, etc

broom::tidy(model, confint=T, exponentiate = F) # change exponentiate to T to get odds ratios for glms

# nifty little trick to amend rounding
broom::tidy(model) %>%
  mutate(across(where(is.numeric), \(x) round(x, 3))) %>%
	 rename("coefficient" = "estimate")

```

Other model outputs and diagnostics
```{r}
coefficients(model)
confint(model)
residuals(model)
```

```{r}
plot(model) # will produce a range of plots
```


```{r}
# predict with new data
new.height = data.frame(height = c(99, 105, 166))
predict(model, newdata = new.height)
```


Get stats for each observation, inc fitted, std. resid, hat, and cookd. 
```{r}
# these stats are simply added as new columns to the original dataset (handy!)
augment(model) %>% head()
```

Model comparison
```{r}
# Compare models
anova(model, model2)

# chi-square (deviance)
anova(m1, test = "Chisq")
```

Regression with `car`
```{r}
# Normality
car::qqPlot(model, labels=row.names(model), id.method="identify",simulate=TRUE, main="Q-Q Plot")

# Independence of errors
car::durbinWatsonTest(model)   # non-sig equals lack of autocorrelation
# lag value=1 means each obs is being compared to one next to it

# Linearity (only available if model has no interactions)
car::crPlots(model)

# Homoscedasticity (check for non-constant error variance)
car::ncvTest(model)
# If non-sig, assumption is met.

car::spreadLevelPlot(model) # looking for a horizontal line

# Multicollinearity
# Variance inflation factor (VIF)
car::vif(model)
# sqrt(VIF) > 2 indicates a potential problem
sqrt(car::vif(model)) > 2

# Outliers
car::outlierTest(model) # locates largest Studentized residual in absolute value and computes Bonferroni-corrected t-test

car::influencePlot(model, id.method="identify", main="Influence Plot",
              sub="Circle size is proportional to Cook's distance")

# Explanation:
# Points above +2 or below -2 on the vertical axis are considered outliers. 
# Points above 0.2 or 0.3 on the horizontal axis have high leverage (unusual 
# combinations of predictor values). Circle size is proportional to influence. 
# Observations depicted by large circles may have disproportionate influence on the 
# parameters estimates of the model.
```

# Linear mixed effects modelling
```{r}
library(lme4) # original lme package
library(lmerTest) # for obtaining p-values for estimates
# When to include as random effect? Harrison (2015) suggests when there are >5 levels. 
# BBolker (https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q4/021094.html) suggests:
# Rules of thumb are >=5 levels for estimating a random effect, and 10-20 observations per parameter

model = lmer(outcome ~ predictor1*predictor2 + (1 | subject), 
            na.action = na.omit, 
            REML = F,  
            control = lmerControl(optimizer = "bobyqa", calc.derivs = T), 
            data=data) # random intercept for subject

model2 = lmer(outcome ~ predictor1*predictor2 + (1 + subject | predictor), ...) # random slope for subject over predictor1

# When adding predictors to previous models, use update
model3 = update(model2, .~. + new_predictor)

# Luke (2017): REML=T is preferred when calculating p-values for each estimate, and
# Satterthwaite or Kenward-Rogers are preferred over likelihood ratio tests or t-as-z (1.96)

summary(model) # gives Satterthwaite p-values by default - requires REML=T
summary(model, ddf="Kenward-Roger") # can also choose "Satterthwaite" or "lme4". 
# Choose Satterthwaite for ML, but choose Kenward Roger for models fit using REML. 

anova(model, model2)

# for tidy output, use the broom.mixed package
broom.mixed::tidy(model) %>% mutate(across(where(is.numeric), round, 5))
# need to use the mutate() function to turn of scientific notation of p-values as
# options(scipen=999) doesn't work.

# if convergence issues:
afex::all_fit(model) # will try with many different optimisers

# To compute LRT for each fixed effect automatically, use
afex::mixed(outcome ~ predictor1*predictor2 + (1|subject), data = data, method="LRT")
afex::mixed(model, dataset, method="LRT")
# the first LRT result will compare the first model with a null model. 
# I think this uses type III sums of squares... by default, anova() uses Type I. 

# Confidence intervals
confint.merMod(model, method="Wald")
# note that the default method is 'profile', but there's also 'boot'(strapping) which takes ages.
# As stated by Bates et al (2015, p35), all 3 produce v. similar results. Wald is just for fixed effects.
# see this? https://cran.r-project.org/web/packages/merTools/vignettes/Using_predictInterval.html 

# Ben Bolker suggests here (https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q4/021094.html)
# that profile = slow but fairly accurate; Wald = fast but least accurate; boot = slowest but most accurate

# diagnostics
# two things: (1) histogram of model residuals
hist(residuals(model)) 
qplot(resid(model)) # as recommended by Linck & Cunnings 2015
plot(resid(model))

# (2) histograms of random effects
# Linck & Cunnings 2015 method:
# Save random effects into object, then plot histograms on lists within it
rfx <- ranef(model)
qplot(rfx$participant$'(Intercept)') # histogram
plot(rfx$participant$'(Intercept)') # for non-constancy
qplot(rfx$school$'age') # for a nested ranef, e.g. (1 + age | school)
lattice::dotplot(ranef(model, condVar = TRUE))

# another method to access intercept values
model.refs <- as.vector(ranef(model)[[1]])[,1]
hist(model.refs)

# pseudo R2
library(MuMIn)
r.squaredGLMM(model)

library(influence.ME) # iteratively deletes subjects or observations to examine influence
model.infl <- influence(model, "code/upn/child/etc")  # computationally intensive- wait!

without.int <- exclude.influence(model, grouping = "group", level = "level")
summary(model)
summary(without.int)
```


### Quantile regression
```{r}
library(quantreg)
m1 = rq(dv ~ iv, tau=c(.05, .25, .5, .75, .95), data=df)
```

## Principal components / factor analysis
```{r}
# Can use PCA as a precursor to factor analysis (covered below), i.e. to choose # of factors
# ---- Note that PCA can be achieved with:
# - pca() / psych::principal()
# - princomp() [base stats], or 
# - prcomp() [base stats]
# Using An Introduction to Applied Multivariate Analysis with R (Everitt & Hothorn)
# They use their own 'MVA' package
library(MVA)

head_dat = boot::frets[, c("l1", "l2")]
cov(head_dat)
head_pca = princomp(x = head_dat)
head_pca
summary(head_pca, loadings = T)

xlim = range(head_pca$scores[,1])
plot(head_pca$scores, xlim=xlim, ylim=xlim)
screeplot(head_pca)

# example with Big5 dataset
# From here: https://www.r-bloggers.com/five-ways-to-calculate-internal-consistency/
# BIG5 dataset can be downloaded here: http://personality-testing.info/_rawdata/BIG5.zip
# Data for 19719 respondents on 50 items (10 for each dimension)


# import data and isolate just the raw scores
setwd("~/Google Drive/STATISTICS/R/R script files")
df = read.csv("big5data.csv", header=T, sep="\t")
df = df[, -c(1:7)]

psych::cortest.bartlett(df) # should be statistically significant
# for oblique (oblimin) rotation, load the GPArotation package
library(GPArotation)
pc1 = psych::principal(df, nfactors = 5, rotate = "oblimin", scores = T)
pc1
# screeplot
plot(pc1$values, type="b")
psych::alpha(df)

# princomp
pca = princomp(df)
summary(pca)
# look for proportion of variance explained 
plot(pca) # choose 3

# Factor analysis ----
fit <- factanal(df, 3, rotation="varimax") # varimax is default
print(fit, digits=2, cutoff=.3, sort=TRUE)
# look at bottom for test of hyp that chosen number of factors is sufficient.
# p < .05 means it's sufficient. 

# extract the factor scores: run it again but specify scores="regression"
scores = factanal(df, 3, rotation="varimax", scores="regression")
# returns a list, so access scores with:
scores$scores

psych::fa()
#for psych, see: https://m-clark.github.io/posts/2020-04-10-psych-explained/ 
```

Another more recent example of FA
```{r}
# isolate data
fa_data = res %>%
  select(yarc.ewr.t1, 
         phab.alliteration,
         phab.blending,
         yarc.lsk,
         da.train.total,
         da.post.total,
         ran.time.total,
         cpm.raw,
         bpvs.raw)
# check for missingness (fine)
fa_data %>% summarise(across(everything(), ~sum(is.na(.) / n())))

# derive correlation matrix
cor_mat <- cor(fa_data)

# assumptions ----
psych::KMO(cor_mat)
# overall MSA = 0.88

# Bartlett test - looking for significant test result (OK)
psych::cortest.bartlett(R = cor_mat, n = nrow(res))

# factor extraction (suggests 2 (with values >1)
eigen(cor_mat)$values

# factor analysis 
# using oblimin for rotation to allow factors to correlate
da_factor <- psych::fa(r = cor_mat, nfactors = 2, fm = "minres", rotate = "oblimin")
da_factor
# print(da_factor, cut = 0.3)

psych::fa.diagram(da_factor$loadings, digits=2)

# export
write.csv(loadings(da_factor), "./Outputs/da_efa_loadings.csv")
write.csv(data.frame((da_factor$communalities)), "./Outputs/da_efa_h2.csv")
```


## Statistical power
```{r}
# Using pwr package. Note: you must leave one of the parameters blank to estimate it, e.g.
# if you want minimum power detectable at b=0.8, leave d as d = NULL
# Permissible values for type = two.sample, one.sample, paired
# Default value of alternative is "two.sided"
pwr::pwr.t.test(d = 0.25, 
                n = 30, 
                power = 0.80,
                sig.level = 0.05, 
                type = "two.sample",
                alternative="two.sided")
```

```{r}
#| eval: true
# For samples of different sizes:
pwr::pwr.t2n.test(n1 = 300, n2 = 350, d= NULL, sig.level = 0.05, power = 0.80)
```
E.g. this says that with sample sizes of 300 and 350, the minimum detectable effect size for 80% power is 0.22. If we want to find out the minimum sample sizes needed for an effect size of 0.15 (698 for each group = 1,396 total).
```{r}
#| eval: true
# For samples of different sizes:
pwr::pwr.t.test(n = NULL, d = 0.15, sig.level = 0.05, power = 0.80, type = "two.sample")
```
Other power analysis packages include `pwrss` (a range of options, inc plots), `BUCSS` (using previously published test statistics for sample size planning), `MBESS` (using confidence limits), `PowerUpR` (for multilevel designs), `webpower` (a range of options), `simglm` (generalised linear mixed models), `Superpower` (simulation for ANOVA designs only). 

```{r}
#| eval: true
# some examples from pwrss - determining minimum sample size
# independent samples
pwrss::pwrss.t.2means(mu1 = 30, mu2 = 28, sd1 = 12, sd2 = 8, kappa = 1, 
               power = .80, alpha = 0.05,
               alternative = "not equal")

# paired samples
pwrss::pwrss.t.2means(mu1 = 30, mu2 = 28, sd1 = 12, sd2 = 8, 
               paired = TRUE, paired.r = 0.50,
               power = .80, alpha = 0.05,
               alternative = "not equal")
```


